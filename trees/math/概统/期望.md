
---
title: 期望
term: Expectation
tag: [](./index.md)
asref: true
---

$\gdef\E{\mathbb{E}}$
$\gdef\spaces#1{~ #1 ~}$
$\gdef\d{\operatorname{d}}$

设离散随机变量 $X$ 的分布律 $P(X=x_k)=p_k$, $X$ 的 [期望](./期望.md) 就是其分布律的加权平均 $\E(X) = \sum_k x_kp_k$. 容易知道 $\E(g(X)) = \sum_k g(x_k)p_k$. 

[期望](./期望.md) 算子 $\E(-)$ 是线性的, 即

- $\E(X+Y) = \E(X) + \E(Y)$. 
- $\E(aX) = a\E(X)$. 

由此立刻推得 $\E(aX+bY) \spaces= a\E(X) + b\E(Y)$. 

由于 [期望](./期望.md) 的加权平均定义, Jensen 不等式可以写为 [期望](./期望.md) 形式. 即, 对于 Borel 可测的凸函数 $g: I \to \R$, 有 $g(\E(x)) \le \E(g(X))$. 

对连续型随机变量 $X$, 设其概率密度函数为 $p(x)$, 其相应的 [期望](./期望.md) 可自然地定义为

$$
\E(X) \spaces= \int_\R x p(x) \d x
$$

相应的也有 $E(g(X)) = \int_\R g(x) p(x) \d x$.

现在我们来证明 $\E X^2 \E Y^2 \ge (\E XY)^2$. 可以考虑 Cauchy 不等式的经典证明之一, 即借由判别式构造二次函数非负. 在概率论的语言中, 这件事有更自然的解释. 考虑随机变量 $U - \lambda V$, 其中 $\lambda$ 为任意实数. 则 $\E((U - \lambda V)^2) \ge 0.$ 展开得到

$$ \E(U^2) - 2\lambda \E UV + \lambda^2 \E(V^2) \spaces\ge 0 $$

判别式 $(2\E UV)^2 - 4\E(U^2)\E(V^2) \le 0$, 证毕. 

